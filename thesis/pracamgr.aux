\relax 
\citation{1}
\citation{2}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Wprowadzenie}{5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{r:wprowadzenie}{{1}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Uczenie z nadzorem a \textit  {concept drift}}{5}}
\newlabel{s:superv}{{1.1}{5}}
\citation{3}
\citation{4}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Schemat uczenia z nadzorem \relax }}{6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{supervised}{{1.1}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Formalizacja problemu}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Paradygmaty uczenia si\k e}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Paradygmaty uczenia w teorii uczenia maszynowego. We wszystkich przypadkach zak\IeC {\l }adamy, \.ze zbi\'or $X_{train}$ jest dost\k epny podczas uczenia, podczas gdy zbi\'or $Y_{test}$ nie jest znany.\relax }}{7}}
\newlabel{t:paradigms}{{1.1}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Ewolucja poj\k e\'c a \textit  {overfitting}}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {1.2}{\ignorespaces \textit  {Concept drift} a \textit  {overfitting} - obni\.zony wynik modelu 1. przy walidacji krzy\.zowej \'swiadczy o przeuczeniu, a nie wyst\k epowaniu ewoluuj\k acych poj\k e\'c. W drugim przypadku model mimo dobrej umiej\k etno\'sci klasyfikacji element\'ow pochodz\k acych z rozk\IeC {\l }adu $D^{source}$, cierpi na spadek jako\'sci przy ewaluacji na zbiorze pochodz\k acym z rozk\IeC {\l }adu $D^{target}$.\relax }}{8}}
\newlabel{t:overfit}{{1.2}{8}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Opis przeprowadzanego eksperymentu}{9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{r:experiment}{{2}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Przedstawienie badanego problemu}{9}}
\newlabel{s:problem}{{2.1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Opis zbioru danych}{9}}
\newlabel{ss:data}{{2.1.1}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Rozmieszczenie czujnik\'ow na ciele stra\.zaka \relax }}{9}}
\newlabel{fireman}{{2.1}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Rozk\IeC {\l }ad klas w zbiorze treningowym \relax }}{10}}
\newlabel{classes}{{2.2}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Ewaluacja jako\'sci klasyfikatora}{11}}
\newlabel{ss:evaluation}{{2.1.2}{11}}
\newlabel{bac}{{2.2}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Opis u\.zytych klasyfikator\'ow}{11}}
\newlabel{s:klasyfikatory}{{2.2}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Klasyfikator liniowy}{11}}
\newlabel{ss:linear}{{2.2.1}{11}}
\@writefile{toc}{\contentsline {subsubsection}{Klasyfikacja oparta na regresji logistycznej (\textit  {logit})}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Wykres funkcji sigmoidalnej \relax }}{12}}
\newlabel{sigmoid}{{2.3}{12}}
\@writefile{toc}{\contentsline {subsubsection}{Maszyna wektor\'ow no\'snych}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces P\IeC {\l }aszczyzna $H_1$ nie rozdziela klas.   P\IeC {\l }aszczyzna $H_2$ rozdziela je, ale z niewielkim marginesem.  P\IeC {\l }aszczyzna $H_3$ rozdziela je z maksymalnym marginesem. \relax }}{12}}
\newlabel{svm}{{2.4}{12}}
\@writefile{toc}{\contentsline {subsubsection}{Klasyfikator "jeden przeciwko wszystkim"}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Lasy losowe}{13}}
\newlabel{ss:random_forest}{{2.2.2}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Przyk\IeC {\l }adowe drzewo decyzyjne \relax }}{13}}
\newlabel{dec_tree}{{2.5}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Przyk\IeC {\l }ad Ewolucji poj\k e\'c w analizowanym zbiorze}{14}}
\newlabel{s:data_concept_drift}{{2.3}{14}}
\citation{6}
\citation{7}
\citation{8}
\citation{9}
\citation{8}
\citation{9}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Ekstrakcja cech z szereg\'ow czasowych}{15}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{c:features}{{3}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Metody klasyfikacji szereg\'ow czasowych}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Proces ekstrakcji cech}{15}}
\newlabel{s:extraction}{{3.2}{15}}
\newlabel{d:series}{{3.2.1}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Reprezentacje pochodne szeregu czasowego}{16}}
\newlabel{ss:representations}{{3.2.1}{16}}
\newlabel{d:repr}{{3.2.2}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Statystyki wyliczane z reprezentacji szeregu}{16}}
\newlabel{ss:feaures}{{3.2.2}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Rozk\IeC {\l }ady o ujemnym (pierwszy wykres) i dodatnim (drugi wykres) wsp\'o\IeC {\l }czynniku sko\'sno\'sci \relax }}{17}}
\newlabel{skew}{{3.1}{17}}
\citation{11}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Kszta\IeC {\l }t rozk\IeC {\l }adu w zale\.zno\'sci od warto\'sci kurtozy \relax }}{18}}
\newlabel{kurthosis}{{3.2}{18}}
\newlabel{d:repr}{{3.2.3}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Przyk\IeC {\l }adowe wykresy danych $(x, y)$ i odpowiadaj\k ace im warto\'sci wsp\'o\IeC {\l }czynnika korelacji liniowej Pearsona \relax }}{20}}
\newlabel{pearson}{{3.3}{20}}
\newlabel{d:peak}{{3.2.4}{20}}
\bibcite{1}{1}
\bibcite{2}{2}
\bibcite{3}{3}
\bibcite{4}{4}
\bibcite{5}{5}
\bibcite{6}{6}
\bibcite{7}{7}
\bibcite{8}{8}
\bibcite{9}{9}
\bibcite{10}{10}
\bibcite{11}{11}
\@writefile{toc}{\contentsline {chapter}{Bibliografia}{21}}
